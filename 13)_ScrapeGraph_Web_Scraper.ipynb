{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/Fine-Tuning/13)_ScrapeGraph_Web_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install package and load the extension\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm\n",
        "\n",
        "# #- Install/run Ollama in terminal\n",
        "\n",
        "# !curl -fsSL https://ollama.com/install.sh | sh\n",
        "# !ollama serve & ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "P4nswaINMG8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvX4nh7H2A2w"
      },
      "outputs": [],
      "source": [
        "#@title Install libraries\n",
        "\n",
        "!pip install -q scrapegraphai\n",
        "!apt install -q chromium-chromedriver\n",
        "!pip install -q nest_asyncio\n",
        "!pip install -q playwright\n",
        "!pip install -q gradio\n",
        "!playwright install\n",
        "\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Graph\n",
        "\n",
        "from scrapegraphai.graphs import SmartScraperGraph, SearchGraph, SpeechGraph\n",
        "from scrapegraphai.utils import convert_to_csv, convert_to_json\n",
        "\n",
        "def run_graph(prompt, config, graph_type, source=None):\n",
        "    if graph_type == \"smart_scraper\":\n",
        "        graph = SmartScraperGraph(\n",
        "            prompt=prompt,\n",
        "            source=source,\n",
        "            config=config\n",
        "        )\n",
        "    elif graph_type == \"search\":\n",
        "        graph = SearchGraph(\n",
        "            prompt=prompt,\n",
        "            config=config\n",
        "        )\n",
        "    elif graph_type == \"speech\":\n",
        "        graph = SpeechGraph(\n",
        "            prompt=prompt,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "    # Run the graph\n",
        "    result = graph.run()\n",
        "\n",
        "    result_format(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "def result_format(result):\n",
        "    convert_to_csv(result, \"result\")\n",
        "    convert_to_json(result, \"result\")"
      ],
      "metadata": {
        "id": "uyN8mSSI3rnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Config\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_KEY')\n",
        "\n",
        "def get_openai_config(model_type, openai_key):\n",
        "    return {\n",
        "        \"llm\": {\n",
        "            \"api_key\": openai_key,\n",
        "            \"model\": model_type,\n",
        "        },\n",
        "        \"tts_model\": {\n",
        "            \"api_key\": openai_key,\n",
        "            \"model\": \"tts-1\",\n",
        "            \"voice\": \"alloy\"\n",
        "        },\n",
        "        \"output_path\": \"audio_summary.mp3\",\n",
        "    }\n",
        "\n",
        "def get_ollama_config(model_type, graph_type):\n",
        "    base_config = {\n",
        "        \"llm\": {\n",
        "            \"model\": f\"ollama/{model_type}\",\n",
        "            \"temperature\": 0.1,\n",
        "            \"format\": \"json\",  # Ollama needs the format to be specified explicitly\n",
        "            \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n",
        "        },\n",
        "        \"embeddings\": {\n",
        "            \"model\": \"ollama/nomic-embed-text\",\n",
        "            \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n",
        "        },\n",
        "        \"verbose\": True,\n",
        "    }\n",
        "    if graph_type == \"search\":\n",
        "        base_config[\"max_results\"] = 5\n",
        "    return base_config\n",
        "\n",
        "def get_gemini_pro_config(graph_type, gemini_pro_key):\n",
        "    base_config = {\n",
        "        \"llm\": {\n",
        "            \"api_key\": gemini_pro_key,\n",
        "            \"model\": \"gemini-pro\",\n",
        "        },\n",
        "    }\n",
        "    if graph_type == \"smart_scraper\":\n",
        "        base_config[\"embeddings\"] = {\n",
        "            \"model\": \"ollama/nomic-embed-text\",\n",
        "            \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n",
        "        }\n",
        "    elif graph_type == \"search\":\n",
        "        base_config.update({\n",
        "            \"temperature\": 0.1,\n",
        "            \"streaming\": True,\n",
        "            \"max_results\": 5,\n",
        "            \"verbose\": True,\n",
        "        })\n",
        "    return base_config\n"
      ],
      "metadata": {
        "id": "khtK6yHM5Dxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main(model, model_type, graph_type, question, source=None):\n",
        "    print(f\"Model: {model}, Model Type: {model_type}, Type: {graph_type}, Question: {question}, Source: {source}\")\n",
        "    try:\n",
        "        graph_config = None\n",
        "        if model == \"ollama\":\n",
        "            graph_config = get_ollama_config(model_type, graph_type)\n",
        "        elif model == \"OpenAI\":\n",
        "            graph_config = get_openai_config(model_type, openai_key)\n",
        "        elif model == \"gemini-pro\":\n",
        "            graph_config = get_gemini_pro_config(graph_type, GOOGLE_API_KEY)\n",
        "\n",
        "        print(f\"Graph Config: {graph_config}\")\n",
        "\n",
        "        result = run_graph(question, graph_config, graph_type, source)\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "PduOeOre7ZbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(\"ollama\", \"llama3\", \"search\", \"Who is the president of the United States?\")"
      ],
      "metadata": {
        "id": "BuTuA21P8OiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}