{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMv7lIznsL5hR+1iPoufAAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/Fine-Tuning/08)_gemma_2b_it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzA8iv3zhk_K"
      },
      "outputs": [],
      "source": [
        "#@title Install libraries\n",
        "\n",
        "!pip install -q -U huggingface_hub\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Log in to HF\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "access_token = userdata.get('HF_TOKEN')\n",
        "login(token=access_token)"
      ],
      "metadata": {
        "id": "NWCYwG-6h38C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Model\n",
        "\n",
        "%%time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Check what type of Device enabled (GPU or CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Load the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "                                        load_in_4bit=True,\n",
        "                                        bnb_4bit_use_double_quant=True,\n",
        "                                        bnb_4bit_quant_type=\"nf4\",\n",
        "                                        bnb_4bit_compute_dtype=torch.bfloat16,)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", quantization_config=quantization_config, low_cpu_mem_usage=True)# Use the model"
      ],
      "metadata": {
        "id": "DzgL-ditiYY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate basic response\n",
        "\n",
        "%%time\n",
        "input_text = \"What is the best thing about Kaggle?\"\n",
        "# Encode input text to PyTorch tensors\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(**input_ids, do_sample=True, max_new_tokens=100, temperature=0.5)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "nUbc-1jki-_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load CSV\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "data.head(2)\n",
        "# data['latitude'][5]"
      ],
      "metadata": {
        "id": "7xlgSc8-jo6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generate summary\n",
        "\n",
        "def generate_summary(ctx):\n",
        "    prompt_template = f\"\"\"Provide summary of following context in 200 words.\n",
        "\n",
        "    Provide only useful information:\n",
        "\n",
        "    Context: {ctx}\"\"\"\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt_template},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    # print(prompt, \"\\n-------------------x-----------------------\")\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    outputs = model.generate(input_ids, max_new_tokens=600)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0])\n",
        "    return response"
      ],
      "metadata": {
        "id": "IRw3U2uPlzem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Regex funtion to post-process the output.\n",
        "\n",
        "import re\n",
        "def extract_content(text):\n",
        "    # Find the index of '<start_of_turn>model'\n",
        "    index = text.find('<start_of_turn>model')\n",
        "\n",
        "    # Extract the content after '<start_of_turn>model'\n",
        "    if index != -1:\n",
        "        content_after_model = text[index + len('<start_of_turn>model'):-5].strip()\n",
        "    else:\n",
        "        return \"Content not found after '<start_of_turn>model'\"\n",
        "    return content_after_model"
      ],
      "metadata": {
        "id": "ojQyAafjmqmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctx = \"\"\"It was a Thursday, but it felt like a Monday to John. And John loved Mondays. He thrived at work. He dismissed the old cliché of dreading Monday mornings and refused to engage in water-cooler complaints about “the grind” and empty conversations that included the familiar parry “How was your weekend?” “Too short!”. Yes, John liked his work and was unashamed.\n",
        "\n",
        "  I should probably get another latte. I’ve just been sitting here with this empty cup. But then I’ll start to get jittery. I’ll get a decaf. No, that’s stupid, it feels stupid to pay for a decaf. I can’t justify that.\n",
        "\n",
        "  John was always impatient on the weekends; he missed the formal structure of the business week. When he was younger he used to stay late after school on Fridays and come in early on Mondays, a pattern his mother referred to with equal parts admiration and disdain as “studying overtime.”\n",
        "\n",
        "  Jesus, I’ve written another loser.\n",
        "\n",
        "  Now, John spent his weekends doing yard work at the Tudor house Rebecca left him after their divorce. Rebecca, with her almond eyes—both in shape and in color—could never be his enemy.\n",
        "\n",
        "  That barista keeps looking at me. She’ll probably ask me to leave if I don’t buy something. She’s kind of attractive. Not her hair—her hair seems stringy—but her face is nice. I should really buy something.\n",
        "\n",
        "  Their divorce was remarkably amicable. In fact, John would often tell his parents, “Rebecca and I are better friends now than when we were married!” In fact, John looked forward to the days when he and Rebecca, with their new partners, would reminisce about their marriage, seeing it in a positive light, like two mature adults.\n",
        "\n",
        "  Maybe I’ll just get a pumpkin-spice loaf. That way I can still sit here without going through a whole production of buying a coffee and giving my name and feeling like an asshole while it gets made.\n",
        "\n",
        "  But if John were being honest, the house did get lonely on the weekends. Rebecca’s parents had been generous enough to leave John the house even though they had paid for it. John was still struggling to get his short-story writing—I mean, his painting—career off the ground, and Rebecca and her family had been more than supportive, even during the breakup.\"\"\"\n",
        "\n",
        "summary = generate_summary(ctx)\n",
        "summary = extract_content(summary)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "kw2r7hnonNQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title beautify the output\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "md(summary)"
      ],
      "metadata": {
        "id": "wq6yd-pQnJ_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}