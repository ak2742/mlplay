{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/main/02)_mistral_7b_with_prompt_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekDiE5-ij8jl"
      },
      "outputs": [],
      "source": [
        "#@title Install all required libraries\n",
        "\n",
        "# Huggingface libraries to run LLM.\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain==0.1.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9n-QYmZEoqj"
      },
      "outputs": [],
      "source": [
        "#@title Check for GPU\n",
        "\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVi2iaAwzUx6"
      },
      "outputs": [],
      "source": [
        "#@title Create Model\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "origin_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig \\\n",
        "              (\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              )\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KQWJfvXzYFo"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(origin_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lH5YUd1l0RD"
      },
      "outputs": [],
      "source": [
        "#@title Creating pipelines to run LLM at Colab notebook\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=100,\n",
        "    temperature = 0.5,\n",
        "    do_sample=True,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2__UIW-l9kt"
      },
      "outputs": [],
      "source": [
        "#@title Run LLM\n",
        "\n",
        "text = \"What is mitochondria?\"\n",
        "response = mistral_llm.invoke(text)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RahrdZximBm6"
      },
      "outputs": [],
      "source": [
        "#@title Add Prompt\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "#### Prompt Template\n",
        "prompt_template = \"\"\"<s>[INST]\n",
        "You are a reliable and trustworthy assistant, providing helpful and\n",
        "respectful responses that precisely address the context.\n",
        "Answer the question below from context below :\n",
        "{context}\n",
        "{question}\n",
        "[/INST]\n",
        "</s>\n",
        "\"\"\"\n",
        "\n",
        "question = \"\"\"What is the primary cause of global warming?\"\"\"\n",
        "context = \"\"\"Global warming refers to the long-term increase in Earth's average\n",
        "surface temperature, primarily caused by human activities, such as the burning of fossil\n",
        "fuels and deforestation. This phenomenon has far-reaching consequences for the planet and its\n",
        "inhabitants. With the release of greenhouse gases into the atmosphere, including carbon dioxide and\n",
        "methane, a thickening blanket is formed, trapping heat and leading to the enhanced greenhouse effect. The\n",
        "consequences of global warming are multifaceted and include rising sea levels, more frequent and severe weather\n",
        "events, shifts in ecosystems, and threats to human health and well-being. Urgent action is required to mitigate global\n",
        "warming by reducing greenhouse gas emissions, transitioning to renewable energy sources, and adopting sustainable practices\n",
        "to safeguard the planet for future generations.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"context\"])\n",
        "\n",
        "llm_chain = prompt | mistral_llm\n",
        "\n",
        "response = llm_chain.invoke({\"question\":question,\"context\":context})\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNbBd11+B8Cr9/Q2xIU086Q",
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
