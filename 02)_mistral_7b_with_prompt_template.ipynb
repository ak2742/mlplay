{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNbBd11+B8Cr9/Q2xIU086Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/main/02)_mistral_7b_with_prompt_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install all required libraries**\n",
        "\n",
        "1. Huggingface libraries to use Mistral 7B LLM (Open Source LLM)\n",
        "\n",
        "2. LangChain library to call LLM to generate reposne based on the prompt"
      ],
      "metadata": {
        "id": "6DDWtFchlXQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekDiE5-ij8jl"
      },
      "outputs": [],
      "source": [
        "# Huggingface libraries to run LLM.\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain==0.1.20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "R9n-QYmZEoqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **We are using Mistral 7 Billion parameter LLM (Open source from HuggingFace)**\n"
      ],
      "metadata": {
        "id": "hzjQ1EI_hA1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "origin_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig \\\n",
        "              (\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              )\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map=\"auto\")"
      ],
      "metadata": {
        "id": "bVi2iaAwzUx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(origin_model_path)"
      ],
      "metadata": {
        "id": "2KQWJfvXzYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating pipelines to run LLM at Colab notebook**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_fWe57MAlzGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=100,\n",
        "    temperature = 0.5,\n",
        "    do_sample=True,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "1lH5YUd1l0RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ask Questions**\n"
      ],
      "metadata": {
        "id": "-84W4c5sl80d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is mitochondria?\"\n",
        "response = mistral_llm.invoke(text)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "q2__UIW-l9kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Add Prompt**"
      ],
      "metadata": {
        "id": "wIDV35fZ7QFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "#### Prompt Template\n",
        "prompt_template = \"\"\"<s>[INST]\n",
        "You are a reliable and trustworthy assistant, providing helpful and\n",
        "respectful responses that precisely address the context.\n",
        "Answer the question below from context below :\n",
        "{context}\n",
        "{question}\n",
        "[/INST]\n",
        "</s>\n",
        "\"\"\"\n",
        "\n",
        "question = \"\"\"What is this about?\"\"\"\n",
        "context = \"\"\"Global warming refers to the long-term increase in Earth's average\n",
        "surface temperature, primarily caused by human activities, such as the burning of fossil\n",
        "fuels and deforestation. This phenomenon has far-reaching consequences for the planet and its\n",
        "inhabitants. With the release of greenhouse gases into the atmosphere, including carbon dioxide and\n",
        "methane, a thickening blanket is formed, trapping heat and leading to the enhanced greenhouse effect. The\n",
        "consequences of global warming are multifaceted and include rising sea levels, more frequent and severe weather\n",
        "events, shifts in ecosystems, and threats to human health and well-being. Urgent action is required to mitigate global\n",
        "warming by reducing greenhouse gas emissions, transitioning to renewable energy sources, and adopting sustainable practices\n",
        "to safeguard the planet for future generations.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"context\"])\n",
        "\n",
        "llm_chain = prompt | mistral_llm\n",
        "\n",
        "response = llm_chain.invoke({\"question\":question,\"context\":context})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "RahrdZximBm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}