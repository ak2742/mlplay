{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1TFaYCUGnLJGCZSoat_xvWJeqcCLrkypX",
      "authorship_tag": "ABX9TyMoJ2Q1+2/Fj8w6UjkUJ2Ry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/models/01)_Bigram_Language_Model_with_AdamW_optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1K0SpTXuC30d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "max_iters = 1000\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250\n"
      ],
      "metadata": {
        "id": "7iqlYApACxR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data from File"
      ],
      "metadata": {
        "id": "-wYKXm6WEq3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/RomeoJuliet.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "Z7YnmsiZDhz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# encoder/decoder to convert text to int"
      ],
      "metadata": {
        "id": "rg3WAz3HGcSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str_to_int = {ch:i for i, ch in enumerate(chars)}\n",
        "int_to_str = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [str_to_int[c] for c in s]\n",
        "decode = lambda l: \"\".join([int_to_str[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "PfPnyHjPE3KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Training(80%) and validation data(20%) and define get_batch"
      ],
      "metadata": {
        "id": "yhBFEwHfJ0Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "nTLM9BOzJP95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# estimate loss function"
      ],
      "metadata": {
        "id": "vDooGzCxmUe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "Ejr_fgXnmTz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# create model"
      ],
      "metadata": {
        "id": "1Dn_IMtBlZ4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLangModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        logits = self.token_embedding_table(index)\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "          # get the predictions\n",
        "          logits, loss = self.forward(index)\n",
        "          # focus only on the last time step\n",
        "          logits = logits[:, -1, :] # becomes (B, C)\n",
        "          # apply softmax to get probabilities\n",
        "          probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "          # sample from the distribution\n",
        "          index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "          # append sampled index to the running sequence\n",
        "          index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "        return index\n",
        "\n",
        "model = BigramLangModel(vocab_size)\n",
        "m = model.to(device)\n"
      ],
      "metadata": {
        "id": "3_1Wd7OYbNDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Optimizer"
      ],
      "metadata": {
        "id": "VbSkXkmoswTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "id": "LNl6RbC6svq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# See results"
      ],
      "metadata": {
        "id": "3EGe_vuokd54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "8SqdwW4VsXTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}