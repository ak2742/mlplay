{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7jiNmYdn10tLnGl/SENUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/PyTorch-Models/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3Fl0jHXASAkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pkHIk8j9BgP"
      },
      "outputs": [],
      "source": [
        "#@title load data\n",
        "\n",
        "text = \"नमस्ते 💖 World!!\"\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/shakespeare.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text += f.read()\n",
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# unicodes = [ord(x) for x in text]\n",
        "# list(txt.encode(\"utf-8\")) # utf-8 utf-16 utf-32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get utf-8 codes for text\n",
        "\n",
        "tokens = text.encode(\"utf-8\")\n",
        "tokens = list(map(int,tokens))\n",
        "print(len(text))\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "Y3LP-xJGSnXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fn to get the n of occurences of a pair in tokens\n",
        "\n",
        "def get_stats(ids):   #{pair -> n_occur}\n",
        "    pairs = {}\n",
        "    for i in range(len(ids) - 1):\n",
        "        pair = tuple(ids[i:i+2])\n",
        "        pairs[pair] = pairs.get(pair, 0) + 1\n",
        "    return pairs\n",
        "\n",
        "# stats = get_stats(tokens)\n",
        "# print(sorted(((v, k) for k, v in stats.items()), reverse=True)[0])\n",
        "\n",
        "# top_pair = max(stats, key=stats.get)\n",
        "# print(top_pair)"
      ],
      "metadata": {
        "id": "f5M1_JrqUe3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title fn to merge a pair into a new token\n",
        "\n",
        "def merge_pairs(ids, pair, idx):\n",
        "  # in a list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "  new_ids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids)-1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i += 1\n",
        "  return new_ids"
      ],
      "metadata": {
        "id": "Qn4f6W6VXnbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title merge tokens upto vocab size\n",
        "\n",
        "vocab_size = 280  # max unique tokens after merge\n",
        "num_merges = vocab_size-256\n",
        "ids = list(tokens)\n",
        "\n",
        "merges = {} # {pair -> new_token}\n",
        "\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  top_pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  ids = merge_pairs(ids, top_pair, idx)\n",
        "  merges[top_pair] = idx\n",
        "\n",
        "merges"
      ],
      "metadata": {
        "id": "RIJyYW3Wga49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens)) # tokens before merges\n",
        "\n",
        "print(len(ids)) # tokens after merges\n",
        "\n",
        "print(f\"compression ratio {len(tokens)/len(ids):.2f}x\")\n"
      ],
      "metadata": {
        "id": "K1KzDwDmhz1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title encoder-decoder\n",
        "\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)} # {tokens -> bytes}\n",
        "for (p0, p1), idx in merges.items():     # for merges\n",
        "  vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "def decode(ids):\n",
        "  tokens = b\"\".join(vocab[i] for i in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "def encode(text):\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) >= 2:\n",
        "    stats = get_stats(tokens)\n",
        "    pair = min(stats, key=lambda k: merges.get(k, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "    idx = merges[pair]\n",
        "    tokens = merge_pairs(tokens, pair, idx)\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "Fvy_nP49i7CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title gpt regex to split text\n",
        "\n",
        "import regex as re\n",
        "reg = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "print(reg.findall(text[:100]))"
      ],
      "metadata": {
        "id": "rUrtplK3u6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title test\n",
        "\n",
        "txt = text\n",
        "encoded_txt = encode(txt)\n",
        "# print(encoded_txt)\n",
        "decoded_txt = decode(encoded_txt)\n",
        "print(decoded_txt[:100])\n",
        "print(decoded_txt == txt)"
      ],
      "metadata": {
        "id": "Xfy3oxkaqni1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}