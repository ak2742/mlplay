{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak2742/mlplay/blob/RAG/04)_Summarize_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC-WfAbZwY8X"
      },
      "outputs": [],
      "source": [
        "#Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpO3pq0CxEZa"
      },
      "outputs": [],
      "source": [
        "# Workaround to avoid following error at notebook\n",
        "# NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewx_vtyTxJ1w"
      },
      "outputs": [],
      "source": [
        "#@title Install all required libraries\n",
        "\n",
        "# Huggingface libraries to run LLM.\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain==0.1.2\n",
        "\n",
        "#and transforming the pages of PDF files\n",
        "!pip install -q -U pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyf3khhwxP_R"
      },
      "outputs": [],
      "source": [
        "#@title imports and Check for GPU\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "\n",
        "import torch\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck6da4IkxgP_"
      },
      "outputs": [],
      "source": [
        "#@title Create Model\n",
        "\n",
        "origin_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig \\\n",
        "              (\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "              )\n",
        "model = AutoModelForCausalLM.from_pretrained (model_path, trust_remote_code=True,\n",
        "                                              quantization_config=bnb_config,\n",
        "                                              device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAdkrtCvZ-69"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(origin_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPptAJHMxpwV"
      },
      "outputs": [],
      "source": [
        "#@title Creating pipelines to run LLM at Colab notebook\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    batch_size=4,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=600,\n",
        "    temperature = 0.3,\n",
        "    do_sample=True,\n",
        ")\n",
        "text_generation_pipeline.tokenizer.pad_token_id = text_generation_pipeline.model.config.eos_token_id\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqFCNNUCxxMx"
      },
      "outputs": [],
      "source": [
        "#@title Install more libraries\n",
        "\n",
        "#Librarires to count token programmatically\n",
        "!pip install -q -U tiktoken\n",
        "!pip install -q -U openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyzvMpV3SPh0"
      },
      "source": [
        "# **Approach 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a99Syw4zx-ke"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "#Count tokens in a string\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    tokens = encoding.encode(string)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    print(f'Chars Count: {len(string)}')\n",
        "    print(f'Token Count: {num_tokens}')\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa5lZeDVF5LM"
      },
      "source": [
        "\n",
        "# Stuff all your documents into a single prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY-BtQP0yEc5"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "\n",
        "def stuff_text_summarization(text):\n",
        "  token_count = num_tokens_from_string(text, \"cl100k_base\")\n",
        "  # print(f'Token Count: {token_count}')\n",
        "\n",
        "  #Converting text to LangChain documents so that StuffDocumentsChain can understand Input\n",
        "  documents = Document(page_content=text, metadata={\"source\": \"local\"})\n",
        "\n",
        "  # Define prompt with prompt template\n",
        "  prompt_template = \"\"\"Write a concise summary of the following:\n",
        "  \"{docs}\"\n",
        "  CONCISE SUMMARY:\"\"\"\n",
        "  prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  # Define LLM chain\n",
        "  llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n",
        "\n",
        "  # Define StuffDocumentsChain\n",
        "  stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"docs\")\n",
        "\n",
        "  #Get the Summary of text by invoking StuffDocumentsChain\n",
        "  summary = stuff_chain.invoke([documents])\n",
        "  return summary['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ek9JE0lyJA-"
      },
      "outputs": [],
      "source": [
        "  text = \"\"\"It was a Thursday, but it felt like a Monday to John. And John loved Mondays. He thrived at work. He dismissed the old cliché of dreading Monday mornings and refused to engage in water-cooler complaints about “the grind” and empty conversations that included the familiar parry “How was your weekend?” “Too short!”. Yes, John liked his work and was unashamed.\n",
        "\n",
        "  I should probably get another latte. I’ve just been sitting here with this empty cup. But then I’ll start to get jittery. I’ll get a decaf. No, that’s stupid, it feels stupid to pay for a decaf. I can’t justify that.\n",
        "\n",
        "  John was always impatient on the weekends; he missed the formal structure of the business week. When he was younger he used to stay late after school on Fridays and come in early on Mondays, a pattern his mother referred to with equal parts admiration and disdain as “studying overtime.”\n",
        "\n",
        "  Jesus, I’ve written another loser.\n",
        "\n",
        "  Now, John spent his weekends doing yard work at the Tudor house Rebecca left him after their divorce. Rebecca, with her almond eyes—both in shape and in color—could never be his enemy.\n",
        "\n",
        "  That barista keeps looking at me. She’ll probably ask me to leave if I don’t buy something. She’s kind of attractive. Not her hair—her hair seems stringy—but her face is nice. I should really buy something.\n",
        "\n",
        "  Their divorce was remarkably amicable. In fact, John would often tell his parents, “Rebecca and I are better friends now than when we were married!” In fact, John looked forward to the days when he and Rebecca, with their new partners, would reminisce about their marriage, seeing it in a positive light, like two mature adults.\n",
        "\n",
        "  Maybe I’ll just get a pumpkin-spice loaf. That way I can still sit here without going through a whole production of buying a coffee and giving my name and feeling like an asshole while it gets made.\n",
        "\n",
        "  But if John were being honest, the house did get lonely on the weekends. Rebecca’s parents had been generous enough to leave John the house even though they had paid for it. John was still struggling to get his short-story writing—I mean, his painting—career off the ground, and Rebecca and her family had been more than supportive, even during the breakup.\"\"\"\n",
        "\n",
        "stuff_summary = stuff_text_summarization(text)\n",
        "\n",
        "print(stuff_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-LRB9yXUYRd"
      },
      "source": [
        "# **Approach 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugtd8uk3yNZ7"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "#Load PDF file and count tokens\n",
        "def load_pdf(file_path):\n",
        "    # Load the pdf file\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    token_count = num_tokens_from_string(str(documents), \"cl100k_base\")\n",
        "\n",
        "    print(f'Docs Count: {len(documents)}')\n",
        "    return documents, token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVuzW3SNG35D"
      },
      "source": [
        "\n",
        "# Split large Docs using Map-Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWyZvuAFyTmg"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, LLMChain, ReduceDocumentsChain, StuffDocumentsChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def map_reduce_summarize_document(file_path):\n",
        "\n",
        "    #Load PDF using PyPDF\n",
        "    documents, token_count = load_pdf(file_path)\n",
        "\n",
        "    # Map Prompt template and LLM Chain\n",
        "    map_prompt_template = \"\"\"[INST] The following is a part of an pdf document:\n",
        "    {docs}\n",
        "    Based on this, please identify the main points.\n",
        "    Answer:  [/INST] \"\"\"\n",
        "    map_prompt = PromptTemplate.from_template(map_prompt_template)\n",
        "    map_chain = LLMChain(llm=mistral_llm, prompt=map_prompt)\n",
        "\n",
        "\n",
        "    # Reduce Prompt template and LLM Chain\n",
        "    reduce_prompt_template = \"\"\"[INST] The following is set of summaries from the article:\n",
        "    {doc_summaries}\n",
        "    Take these and distill it into a final, consolidated summary of the main points.\n",
        "    Construct it as a well organized summary of the main points and should be between 3 and 5 paragraphs.\n",
        "    Answer:  [/INST] \"\"\"\n",
        "    reduce_prompt = PromptTemplate.from_template(reduce_prompt_template)\n",
        "    reduce_chain = LLMChain(llm=mistral_llm, prompt=reduce_prompt)\n",
        "\n",
        "\n",
        "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "    combine_documents_chain = StuffDocumentsChain(\n",
        "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        "    )\n",
        "\n",
        "    # Combines and iteratively reduces the mapped documents\n",
        "    reduce_documents_chain = ReduceDocumentsChain(\n",
        "        # This is final chain that is called.\n",
        "        combine_documents_chain=combine_documents_chain,\n",
        "        # If documents exceed context for `StuffDocumentsChain`\n",
        "        collapse_documents_chain=combine_documents_chain,\n",
        "        # The maximum number of tokens to group documents into.\n",
        "        token_max=4000,\n",
        "    )\n",
        "\n",
        "    # Combining documents by mapping a chain over them, then combining results\n",
        "    map_reduce_chain = MapReduceDocumentsChain(\n",
        "        # Map chain\n",
        "        llm_chain=map_chain,\n",
        "        # Reduce chain\n",
        "        reduce_documents_chain=reduce_documents_chain,\n",
        "        # The variable name in the llm_chain to put the documents in\n",
        "        document_variable_name=\"docs\",\n",
        "        # Return the results of the map steps in the output\n",
        "        return_intermediate_steps=True,\n",
        "    )\n",
        "\n",
        "    # Split documents into chunks using RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=4000, chunk_overlap=100\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f'Split Docs Count: {len(split_docs)}')\n",
        "    # Run the chain\n",
        "    result = map_reduce_chain.invoke(split_docs, return_only_outputs=False)\n",
        "    return result['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax495QllyYEJ"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/ImpactofIndianPremierLeagueonTestCricketinIndia.pdf'\n",
        "\n",
        "summary_response = map_reduce_summarize_document(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH9e7xcGybQr"
      },
      "outputs": [],
      "source": [
        "print(summary_response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOYmUj/Amgj5Tmav6dK1zpR",
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
